# configs/cifar100.yaml

project_name: "boost-self-supervised-dataset-distillation"
experiment_name: "cifar100_distillation"

data:
  name: "CIFAR100"
  path: "./data"
  resolution: [3, 32, 32]
  num_classes: 100
  full_dataset_size: 50000

distillation:
  storage_budget_N: 100
  num_distilled_images_m: 100
  steps: 30000
  optimizer:
    name: "AdamW"
    lr: 0.001
    decay_type: "linear"

parametrization:
  image_bases_U: 200
  repr_bases_V: 200
  image_basis_size: 16
  upsampling_scale: 2

models:
  teacher:
    arch: "ResNet18"
    path: "./teacher_models/resnet18_barlow_twins_cifar100.pth"
    feature_dim: 512
  inner_cnn:
    channels: [128, 256, 512]
    feature_dim: 512
  approximation_mlp:
    hidden_dim: 4

augmentations:
  # Optimal augmentation parameters (replaces predefined rotations)
  use_optimal: true  # Set to false to use predefined rotations
  num_augmentations: 3  # Number of optimal augmentations to generate
  kernel_type: "rbf"  # Options: 'rbf', 'linear', 'polynomial'
  kernel_params:  # Kernel-specific parameters
    gamma: 1.0e-5  # For RBF kernel
  lambda_ridge: 1.0  # Ridge regularization parameter
  mu_p: 1.0  # Pre-image solver parameter
  
  # Legacy predefined augmentation (for backward compatibility)
  rotate: [90, 180, 270]  # Only used if use_optimal is false

saving:
  distilled_assets_dir: "./distilled_assets/cifar100_N100"

model_pool:
  size_L: 10
  inner_loop_steps_Z: 1000

evaluation:
  epochs: 1000
  lr: 0.1
  weight_decay: 1e-4
  # Linear evaluation settings
  linear_epochs: 100
  linear_lr: 0.2