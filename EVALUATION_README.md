# Evaluation Guide: Full Dataset vs Distilled Dataset Comparison

This guide explains how to evaluate and compare ResNet18 models trained on the full CIFAR100 dataset versus the distilled dataset generated by the self-supervised dataset distillation method.

## Overview

Based on the paper "Boost Self-Supervised Dataset Distillation via Parameterization, Predefined Augmentation, and Approximation", this evaluation framework compares:

1. **Self-Supervised Learning Evaluation**: Train ResNet18 as feature extractors using MSE loss, then perform linear evaluation
2. **Classification Evaluation**: Train ResNet18 classifiers directly using cross-entropy loss

## Prerequisites

1. **Generate Distilled Dataset**: First, run the distillation process:
   ```bash
   python main_distill.py
   ```
   This will create distilled assets in `./distilled_assets/cifar100_N100/`

2. **Required Files**: Ensure the following files exist:
   - `distilled_assets/cifar100_N100/distilled_data.pth`
   - `distilled_assets/cifar100_N100/approx_net_rot_90.pth`
   - `distilled_assets/cifar100_N100/approx_net_rot_180.pth`
   - `distilled_assets/cifar100_N100/approx_net_rot_270.pth`

3. **Teacher Model**: Ensure the teacher model exists at:
   - `teacher_models/resnet18_barlow_twins_cifar100.pth`

## Evaluation Methods

### Method 1: Complete Evaluation (Recommended)

Run both self-supervised and classification evaluations:

```bash
python run_evaluation.py --mode both
```

### Method 2: Self-Supervised Learning Evaluation Only

This approach follows the paper's methodology:
- Train ResNet18 feature extractors using MSE loss to mimic teacher representations
- Perform linear evaluation on CIFAR100 test set

```bash
python run_evaluation.py --mode ssl
# or directly:
python main_evaluate.py
```

### Method 3: Classification Evaluation Only

This approach trains ResNet18 classifiers directly:
- Train on full CIFAR100 with cross-entropy loss
- Train on distilled dataset with pseudo-labels

```bash
python run_evaluation.py --mode classification
# or directly:
python pretrain_classification.py
```

## Expected Results

### Self-Supervised Learning Metrics
- **Accuracy Retention**: How well the distilled dataset preserves the teacher's representation quality
- **Training Time**: Time to train feature extractors
- **Linear Evaluation Performance**: Final accuracy on CIFAR100 test set

### Classification Metrics
- **Test Accuracy**: Direct classification performance on CIFAR100
- **Training Efficiency**: Speed improvement from using distilled data
- **Dataset Compression**: 100 distilled images vs 50,000 original images (500x compression)

### Key Metrics to Monitor

1. **Accuracy Retention**: `distilled_accuracy / full_accuracy`
   - Target: >80% (excellent), >60% (good)

2. **Training Speed-up**: `full_training_time / distilled_training_time`
   - Expected: 10-100x faster training

3. **Storage Efficiency**: 
   - Dataset size: 100 vs 50,000 images (500x compression)
   - Model convergence with much less data

## Understanding the Results

### Good Results Indicators:
- **High Accuracy Retention** (>80%): Distilled dataset preserves most information
- **Significant Speed-up** (>10x): Training is much faster
- **Stable Training**: Models converge reliably on distilled data

### Analysis Questions:
1. How much accuracy do we sacrifice for the massive dataset compression?
2. Is the training speed-up worth the accuracy trade-off?
3. Which evaluation method (SSL vs classification) better demonstrates the distilled dataset quality?

## Troubleshooting

### Error: "Distilled assets not found"
```bash
# Run distillation first
python main_distill.py
```

### Error: "Teacher model not found"
```bash
# Pre-train teacher model
python pretrain.py
```

### CUDA Out of Memory
- Reduce batch sizes in the evaluation scripts
- Use smaller models for inner CNN if needed

### Poor Results
- Check if distillation completed successfully
- Verify teacher model quality
- Consider adjusting distillation hyperparameters

## Output Files

After evaluation, you'll find:
- `evaluation_models/`: Trained models for comparison
- `classification_models/`: Classification models
- `evaluation_results.txt`: Summary of all results

## Paper Reference

This evaluation implements the methodology from:
```
"Boost Self-Supervised Dataset Distillation via Parameterization, Predefined Augmentation, and Approximation"
arXiv:2507.21455v2 [cs.CV]
```

The evaluation validates the key claims:
- Efficient dataset compression while preserving performance
- Cross-architecture generalization capability
- Superior distillation efficiency compared to baselines

## Configuration

Modify `configs/cifar100.yaml` to adjust:
- Number of distilled images (`num_distilled_images_m`)
- Storage budget (`storage_budget_N`)
- Model architectures and hyperparameters
